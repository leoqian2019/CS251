{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Leo Qian**\n",
    "\n",
    "Fall 2021\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Project 6: Supervised learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Preprocess full spam email dataset \n",
    "\n",
    "Before you build a Naive Bayes spam email classifier, run the full spam email dataset through your preprocessing code.\n",
    "\n",
    "Download and extract the full **Enron** emails (*zip file should be ~29MB large*). You should see a base `enron` folder, with `spam` and `ham` subfolders when you extract the zip file (these are the 2 classes).\n",
    "\n",
    "Run the test code below to check everything over."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3a) Preprocess dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import email_preprocessor as epp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test `count_words` and `find_top_words`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "word_freq, num_emails = epp.count_words()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 32625.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You found 32625 emails in the datset. You should have found 32625.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "print(f\"Your top 5 words are\\n{top_words[:5]}\\nand they should be\\n['the', 'to', 'and', 'of', 'a']\")\n",
    "print(f\"The associated counts are\\n{top_counts[:5]}\\nand they should be\\n[277459, 203659, 148873, 139578, 111796]\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your top 5 words are\n",
      "['the', 'to', 'and', 'of', 'a']\n",
      "and they should be\n",
      "['the', 'to', 'and', 'of', 'a']\n",
      "The associated counts are\n",
      "[277459, 203659, 148873, 139578, 111796]\n",
      "and they should be\n",
      "[277459, 203659, 148873, 139578, 111796]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3b) Make train and test splits of the dataset\n",
    "\n",
    "Here we divide the email features into a 80/20 train/test split (80% of data used to train the supervised learning model, 20% we withhold and use for testing / prediction)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "features, y = epp.make_feature_vectors(top_words, num_emails)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "np.random.seed(0)\n",
    "x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(features, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "print('Shapes for train/test splits:')\n",
    "print(f'Train {x_train.shape}, classes {y_train.shape}')\n",
    "print(f'Test {x_test.shape}, classes {y_test.shape}')\n",
    "print('\\nThey should be:\\nTrain (26100, 200), classes (26100,)\\nTest (6525, 200), classes (6525,)')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes for train/test splits:\n",
      "Train (26100, 200), classes (26100,)\n",
      "Test (6525, 200), classes (6525,)\n",
      "\n",
      "They should be:\n",
      "Train (26100, 200), classes (26100,)\n",
      "Test (6525, 200), classes (6525,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3c) Save data in binary format\n",
    "\n",
    "It adds a lot of overhead to have to run through your raw email -> train/test feature split every time you wanted to work on your project! In this step, you will export the data in memory to disk in a binary format. That way, you can quickly load all the data back into memory (directly in ndarray format) whenever you want to work with it again. No need to parse from text files!\n",
    "\n",
    "- Use numpy's `save` function to make six files in `.npy` format (e.g. `email_train_x.npy`, `email_train_y.npy`, `email_train_inds.npy`, `email_test_x.npy`, `email_test_y.npy`, `email_test_inds.npy`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "np.save('data/email_train_x.npy', x_train)\n",
    "np.save('data/email_train_y.npy', y_train)\n",
    "np.save('data/email_train_inds.npy', inds_train)\n",
    "np.save('data/email_test_x.npy', x_test)\n",
    "np.save('data/email_test_y.npy', y_test)\n",
    "np.save('data/email_test_inds.npy', inds_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4: Naive Bayes Classifier\n",
    "\n",
    "After finishing your email preprocessing pipeline, implement the one other supervised learning algorithm we we will use to classify email, **Naive Bayes**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4a) Implement Naive Bayes\n",
    "\n",
    "In `naive_bayes_multinomial.py`, implement the following methods:\n",
    "- Constructor\n",
    "- `train(data, y)`: Train the Naive Bayes classifier so that it records the \"statistics\" of the training set: the log of the class priors (i.e. how likely an email is in the training set to be spam or ham?) and the log of the class likelihoods (the probability of a word appearing in each class â€” spam or ham).\n",
    "- `predict(data)`: Combine the log class likelihoods and log priors to compute the log posterior distribution. The predicted class for a test sample is the class that yields the highest posterior probability.\n",
    "- `accuracy(y, y_pred)`: The usual definition :)\n",
    "\n",
    "\n",
    "#### Bayes rule ingredients: Priors and likelihood (`train`)\n",
    "\n",
    "To compute class predictions (probability that a test example belong to either spam or ham classes), we need to evaluate **Bayes Rule**. This means computing the priors and likelihoods based on the training data. We store the log of the priors and the log of the likelihoods.\n",
    "\n",
    "**Prior:** $$P_c = \\frac{N_c}{N}$$ where $P_c$ is the prior for class $c$ (spam or ham), $N_c$ is the number of training samples that belong to class $c$ and $N$ is the total number of training samples.\n",
    "\n",
    "**Likelihood:** $$L_{c,w} = \\frac{N_{c,w} + 1}{N_{c} + M}$$ where\n",
    "- $L_{c,w}$ is the likelihood that word $w$ belongs to class $c$ (*i.e. what we are solving for*)\n",
    "- $N_{c,w}$ is the total count of **word $w$** in emails that are only in class $c$ (*either spam or ham*)\n",
    "- $N_{c}$ is the total number of **all words** that appear in emails of the class $c$ (*total number of words in all spam emails or total number of words in all ham emails*)\n",
    "- $M$ is the number of features (*number of top words*).\n",
    "\n",
    "Store $Log(P_c)$ and $Log(L_{c,w})$ in the `log_class_priors` and `log_class_likelihoods` fields.\n",
    "\n",
    "#### Bayes rule ingredients: Posterior (`predict`)\n",
    "\n",
    "To make predictions, we now combine the prior and likelihood to get the posterior:\n",
    "\n",
    "**Log Posterior:** $$Log(\\text{Post}_{i, c}) = Log(P_c) + \\sum_{j \\in J_i}x_{i,j}Log(L_{c,j})$$\n",
    "\n",
    " where\n",
    "- $\\text{Post}_{i,c}$ is the posterior for class $c$ for test sample $i$(*i.e. evidence that email $i$ is spam or ham*). We solve for its logarithm.\n",
    "- $Log(P_c)$ is the logarithm of the prior for class $c$.\n",
    "- $x_{i,j}$ is the number of times the jth word appears in the ith email.\n",
    "- $Log(L_{c,j})$: is the log-likelihood of the jth word in class $c$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from naive_bayes_multinomial import NaiveBayes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test `train`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.random(size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your log class priors are: {nbc.log_class_priors}\\nand should be          [-1.42712 -1.34707 -1.38629 -1.38629].')\n",
    "print(f'Your log class likelihoods shape is {nbc.log_class_likelihoods.shape} and should be (4, 6).')\n",
    "print(f'Your log likelihoods are:\\n{nbc.log_class_likelihoods}')\n",
    "\n",
    "\n",
    "test_likelihoods = np.array(\n",
    "[[-1.88939, -1.68757, -1.73893, -1.92213, -1.78302, -1.75022],\n",
    " [-1.79005, -1.7466,  -1.84883, -1.7786,  -1.85297, -1.73946],\n",
    " [-1.95787, -1.85664, -1.62705, -1.76927, -1.71753, -1.85681],\n",
    " [-1.67787, -1.70203, -1.83983, -2.09846, -1.78647, -1.70444]])\n",
    "print(f'and should be\\n{test_likelihoods}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your log class priors are: [-1.42712 -1.34707 -1.38629 -1.38629]\n",
      "and should be          [-1.42712 -1.34707 -1.38629 -1.38629].\n",
      "Your log class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your log likelihoods are:\n",
      "[[-1.88939 -1.68757 -1.73893 -1.92213 -1.78302 -1.75022]\n",
      " [-1.79005 -1.7466  -1.84883 -1.7786  -1.85297 -1.73946]\n",
      " [-1.95787 -1.85664 -1.62705 -1.76927 -1.71753 -1.85681]\n",
      " [-1.67787 -1.70203 -1.83983 -2.09846 -1.78647 -1.70444]]\n",
      "and should be\n",
      "[[-1.88939 -1.68757 -1.73893 -1.92213 -1.78302 -1.75022]\n",
      " [-1.79005 -1.7466  -1.84883 -1.7786  -1.85297 -1.73946]\n",
      " [-1.95787 -1.85664 -1.62705 -1.76927 -1.71753 -1.85681]\n",
      " [-1.67787 -1.70203 -1.83983 -2.09846 -1.78647 -1.70444]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test `predict`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_train = np.random.randint(low=0, high=num_test_classes, size=(100, 10))\n",
    "data_test = np.random.randint(low=0, high=num_test_classes, size=(15, 10))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_train, y_test)\n",
    "test_y_pred = nbc.predict(data_test)\n",
    "\n",
    "print(f'Your predicted classes are\\n{test_y_pred}\\nand should be\\n[3 0 3 1 0 1 1 3 0 3 0 2 0 2 1]')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your predicted classes are\n",
      "[3 0 3 1 0 1 1 3 0 3 0 2 0 2 1]\n",
      "and should be\n",
      "[3 0 3 1 0 1 1 3 0 3 0 2 0 2 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4b) Spam filtering\n",
    "\n",
    "Let's start classifying spam email using the Naive Bayes classifier.\n",
    "\n",
    "- Use `np.load` to load in the train/test split that you created last week.\n",
    "- Use your Naive Bayes classifier on the Enron email dataset!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 7:** Print out the accuracy that you get on the test set with Naive Bayes. It should be roughly 89%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "import email_preprocessor as ep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "x_train = np.load('data/email_train_x.npy')\n",
    "y_train = np.load('data/email_train_y.npy')\n",
    "inds_train = np.load('data/email_train_inds.npy')\n",
    "x_test = np.load('data/email_test_x.npy')\n",
    "y_test = np.load('data/email_test_y.npy')\n",
    "inds_test = np.load('data/email_test_inds.npy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# Write your Naive Bayes code here\n",
    "naive_classifier = NaiveBayes(2)\n",
    "\n",
    "naive_classifier.train(x_train, y_train.astype(int))\n",
    "\n",
    "y_pred = naive_classifier.predict(x_test)\n",
    "\n",
    "accur = naive_classifier.accuracy(y_test, y_pred)\n",
    "\n",
    "print(f'accuracy: {accur:.2f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy: 0.90\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4c) Confusion matrix\n",
    "\n",
    "To get a better sense of the errors that the Naive Bayes classifer makes, you will create a confusion matrix. \n",
    "\n",
    "- Implement `confusion_matrix` in `naive_bayes.py`.\n",
    "- Print out a confusion matrix of the spam classification results.\n",
    "\n",
    "**Debugging guidelines**:\n",
    "1. The sum of all numbers in your 2x2 confusion matrix should equal the number of test samples (6525).\n",
    "2. The sum of your spam row should equal the number of spam samples in the test set (3193)\n",
    "3. The sum of your ham row should equal the number of spam samples in the test set (3332)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "confusion = naive_classifier.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "# count = {}\n",
    "# count[0] = 0\n",
    "# count[1] = 0\n",
    "# for label in y_test:\n",
    "#     count[int(label)] += 1\n",
    "\n",
    "print(count)\n",
    "\n",
    "\n",
    "print(confusion)\n",
    "print(confusion.sum())\n",
    "print(confusion.sum(axis = 1,keepdims=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 3244, 1: 3281}\n",
      "[[3119.  125.]\n",
      " [ 497. 2784.]]\n",
      "6525.0\n",
      "[[3244.]\n",
      " [3281.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 8:** Interpret the confusion matrix, using the convention that positive detection means spam (*e.g. a false positive means classifying a ham email as spam*). What types of errors are made more frequently by the classifier? What does this mean (*i.e. X (spam/ham) is more likely to be classified than Y (spam/ham) than the other way around*)?\n",
    "\n",
    "**Reminder:** Look back and make sure you are clear on which class indices correspond to spam/ham."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer 8:** look like more hams are classified as spam comparing to the number of spam classified as ham. This means there are probabilily some emails in the ham folder with a lot of words that appears more in the spam emails of the traning dataset so that these hams are classified as spam. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 5: Comparison with KNN\n",
    "\n",
    "\n",
    "- Run a similar analysis to what you did with Naive Bayes above. When computing accuracy on the test set, you may want to reduce the size of the test set (e.g. to the first 500 emails in the test set).\n",
    "- Copy-paste your `confusion_matrix` method into `knn.py` so that you can run the same analysis on a KNN classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "from knn import KNN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "knn_classifier = KNN(2)\n",
    "knn_classifier.train(x_train, y_train.astype(int))\n",
    "y_pred = knn_classifier.predict(x_test[:500],2)\n",
    "\n",
    "accur = knn_classifier.accuracy(y_test[:500], y_pred)\n",
    "\n",
    "print(f'accuracy: {accur:.2f}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy: 0.90\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "print(knn_classifier.confusion_matrix(y_test[:500],y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[221.   5.]\n",
      " [ 43. 231.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 9:** What accuracy did you get on the test set (potentially reduced in size)?\n",
    "\n",
    "**Question 10:** How does the confusion matrix compare to that obtained by Naive Bayes (*If you reduced the test set size, keep that in mind*)?\n",
    "\n",
    "**Question 11:** Briefly describe at least one pro/con of KNN compared to Naive Bayes on this dataset.\n",
    "\n",
    "**Question 12:** When potentially reducing the size of the test set here, why is it important that we shuffled our train and test set?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer 9:** I got a very good accuracy, like 90%\n",
    "\n",
    "**Answer 10:**  the confusion matrix looks almost as good as the one for the naive bay classifier which slightly better results proportionally\n",
    "\n",
    "**Answer 11:** knn may give better result but which a great cost of speed\n",
    "\n",
    "**Answer 12:** in this case, we will try our best to make the training and test be separated since we want to have an accurate result of the performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extensions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0. Classify your own datasets\n",
    "\n",
    "- Find datasets that you find interesting and run classification on them using your KNN algorithm (and if applicable, Naive Bayes). Analysis the performance of your classifer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "heart_data = np.loadtxt(\"data/heart_data.csv\",skiprows=1,delimiter=\",\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "x_train,y_train,inds_train,x_test,y_test,inds_test = epp.make_train_test_sets(heart_data[:,:5],heart_data[:,-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "knn_classifier = KNN(2)\n",
    "knn_classifier.train(x_train,y_train)\n",
    "y_pred = knn_classifier.predict(x_test,50)\n",
    "accur = knn_classifier.accuracy(y_test,y_pred)\n",
    "\n",
    "print(f'knn classifier accuracy: {accur:.2f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "knn classifier accuracy: 0.68\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "naive_classifier = NaiveBayes(2)\n",
    "naive_classifier.train(x_train,y_train.astype(int))\n",
    "y_pred = naive_classifier.predict(x_test)\n",
    "accur = naive_classifier.accuracy(y_test.astype(int),y_pred.astype(int))\n",
    "\n",
    "print(f'naive bayes classifier accuracy: {accur:.2f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "naive bayes classifier accuracy: 0.59\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the result above is understandable. The low accuracy is probabily because even though we have some data related to people either with or without heart disease, but those features are not the causing factors for the label: whether the person have heart disease\n",
    "\n",
    "For the fact that knn classifier works better than the naive bay classifier, I believe that's because people with or without heart disease may have similar heart-related conditions so that we would be able to predict their heart disease state by refering to their neighbors. But, still, the accuracy is pretty low meaning the data we chose to predict the data is not sufficient.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Better text preprocessing\n",
    "\n",
    "- If you look at the top words extracted from the email dataset, many of them are common \"stop words\" (e.g. a, the, to, etc.) that do not carry much meaning when it comes to differentiating between spam vs. non-spam email. Improve your preprocessing pipeline by building your top words without stop words. Analyze performance differences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Feature size\n",
    "\n",
    "- Explore how the number of selected features for the email dataset influences accuracy and runtime performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Distance metrics\n",
    "- Compare KNN performance with the $L^2$ and $L^1$ distance metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. K-Fold Cross-Validation\n",
    "\n",
    "- Research this technique and apply it to data and your KNN and/or Naive Bayes classifiers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Email error analysis\n",
    "\n",
    "- Dive deeper into the properties of the emails that were misclassified (FP and/or FN) by Naive Bayes or KNN. What is their word composition? How many words were skipped because they were not in the training set? What could plausibly account for the misclassifications?"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}